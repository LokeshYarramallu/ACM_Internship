{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "currently this is our implementation as of now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is a small trail to know hoe deep neural networks works for predicting dosha\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we contact ayurveda dept. and currently on the way to collect data to predict the dosha from them for training  purposes. also they sharead a process of finding dosha thouthg input score\n",
    "\n",
    "Now the mail goal diverts to the point of making #D models as required form teh input of user and intergrating it with the backend for the final achievement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LOKESH\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your data\n",
    "data = pd.read_csv('data1.csv')\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop(columns=['C'])\n",
    "y = data['C']\n",
    "\n",
    "# One-hot encode categorical features\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "X_encoded = encoder.fit_transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.15, random_state=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42      Vata\n",
       "86     Kapha\n",
       "83      Vata\n",
       "101    Kapha\n",
       "38     Kapha\n",
       "68      Vata\n",
       "103    Kapha\n",
       "59     Pitta\n",
       "99      Vata\n",
       "111     Vata\n",
       "143     Vata\n",
       "8      Kapha\n",
       "35     Kapha\n",
       "132     Vata\n",
       "1      Kapha\n",
       "5      Pitta\n",
       "20     Kapha\n",
       "67      Vata\n",
       "0       Vata\n",
       "32     Kapha\n",
       "24      Vata\n",
       "74      Vata\n",
       "66      Vata\n",
       "Name: C, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "# Use LabelEncoder to encode the categorical target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Now, y_train_encoded and y_test_encoded contain numerical labels\n",
    "\n",
    "# Define CustomDataset class with encoded target variable\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)  # Use torch.long for integer labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Create instances of CustomDataset\n",
    "train_dataset = CustomDataset(X_train, y_train_encoded)\n",
    "test_dataset = CustomDataset(X_test, y_test_encoded)\n",
    "\n",
    "# DataLoader remains the same\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)  # Adjust input size as per your data\n",
    "        self.fc2 = nn.Linear(64, output_size)  # Adjust output size as per your data\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = MyModel(input_size=X_train.shape[1], output_size=len(set(y_train)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.0004\n",
      "Epoch [11/100], Loss: 0.0003\n",
      "Epoch [21/100], Loss: 0.0003\n",
      "Epoch [31/100], Loss: 0.0002\n",
      "Epoch [41/100], Loss: 0.0002\n",
      "Epoch [51/100], Loss: 0.0002\n",
      "Epoch [61/100], Loss: 0.0001\n",
      "Epoch [71/100], Loss: 0.0001\n",
      "Epoch [81/100], Loss: 0.0001\n",
      "Epoch [91/100], Loss: 0.0001\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    if epoch%10 == 0 : print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: Vata, Actual: Vata\n",
      "Predicted: Kapha, Actual: Kapha\n",
      "Predicted: Vata, Actual: Vata\n",
      "Predicted: Kapha, Actual: Kapha\n",
      "Predicted: Kapha, Actual: Kapha\n",
      "Predicted: Vata, Actual: Vata\n",
      "Predicted: Kapha, Actual: Kapha\n",
      "Predicted: Vata, Actual: Pitta\n",
      "Predicted: Vata, Actual: Vata\n",
      "Predicted: Vata, Actual: Vata\n",
      "Predicted: Pitta, Actual: Vata\n",
      "Predicted: Kapha, Actual: Kapha\n",
      "Predicted: Kapha, Actual: Kapha\n",
      "Predicted: Vata, Actual: Vata\n",
      "Predicted: Kapha, Actual: Kapha\n",
      "Predicted: Pitta, Actual: Pitta\n",
      "Predicted: Kapha, Actual: Kapha\n",
      "Predicted: Vata, Actual: Vata\n",
      "Predicted: Vata, Actual: Vata\n",
      "Predicted: Kapha, Actual: Kapha\n",
      "Predicted: Vata, Actual: Vata\n",
      "Predicted: Vata, Actual: Vata\n",
      "Predicted: Vata, Actual: Vata\n",
      "Accuracy Score:  0.9130434782608695\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Lists to store predicted and actual labels\n",
    "predicted_labels = []\n",
    "actual_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        # Convert predicted labels back to original categorical values\n",
    "        predicted_labels.extend(label_encoder.inverse_transform(predicted.numpy()))\n",
    "        actual_labels.extend(label_encoder.inverse_transform(labels.numpy()))\n",
    "\n",
    "# Print predicted and actual labels\n",
    "for predicted_label, actual_label in zip(predicted_labels, actual_labels):\n",
    "    print(f\"Predicted: {predicted_label}, Actual: {actual_label}\")\n",
    "\n",
    "\n",
    "#accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(actual_labels, predicted_labels)\n",
    "print(\"Accuracy Score: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: Vata, Actual: Vata\n",
      "Output Probabilities:\n",
      "Kapha: 0.0001\n",
      "Pitta: 0.0003\n",
      "Vata: 0.9997\n",
      "\n",
      "Predicted: Kapha, Actual: Kapha\n",
      "Output Probabilities:\n",
      "Kapha: 0.9785\n",
      "Pitta: 0.0177\n",
      "Vata: 0.0038\n",
      "\n",
      "Predicted: Vata, Actual: Vata\n",
      "Output Probabilities:\n",
      "Kapha: 0.0001\n",
      "Pitta: 0.0002\n",
      "Vata: 0.9998\n",
      "\n",
      "Predicted: Kapha, Actual: Kapha\n",
      "Output Probabilities:\n",
      "Kapha: 1.0000\n",
      "Pitta: 0.0000\n",
      "Vata: 0.0000\n",
      "\n",
      "Predicted: Kapha, Actual: Kapha\n",
      "Output Probabilities:\n",
      "Kapha: 1.0000\n",
      "Pitta: 0.0000\n",
      "Vata: 0.0000\n",
      "\n",
      "Predicted: Vata, Actual: Vata\n",
      "Output Probabilities:\n",
      "Kapha: 0.0001\n",
      "Pitta: 0.4746\n",
      "Vata: 0.5254\n",
      "\n",
      "Predicted: Kapha, Actual: Kapha\n",
      "Output Probabilities:\n",
      "Kapha: 0.9991\n",
      "Pitta: 0.0008\n",
      "Vata: 0.0000\n",
      "\n",
      "Predicted: Vata, Actual: Pitta\n",
      "Output Probabilities:\n",
      "Kapha: 0.0000\n",
      "Pitta: 0.0000\n",
      "Vata: 1.0000\n",
      "\n",
      "Predicted: Vata, Actual: Vata\n",
      "Output Probabilities:\n",
      "Kapha: 0.0000\n",
      "Pitta: 0.0000\n",
      "Vata: 1.0000\n",
      "\n",
      "Predicted: Vata, Actual: Vata\n",
      "Output Probabilities:\n",
      "Kapha: 0.0000\n",
      "Pitta: 0.0000\n",
      "Vata: 1.0000\n",
      "\n",
      "Predicted: Pitta, Actual: Vata\n",
      "Output Probabilities:\n",
      "Kapha: 0.0018\n",
      "Pitta: 0.5711\n",
      "Vata: 0.4271\n",
      "\n",
      "Predicted: Kapha, Actual: Kapha\n",
      "Output Probabilities:\n",
      "Kapha: 1.0000\n",
      "Pitta: 0.0000\n",
      "Vata: 0.0000\n",
      "\n",
      "Predicted: Kapha, Actual: Kapha\n",
      "Output Probabilities:\n",
      "Kapha: 0.9901\n",
      "Pitta: 0.0000\n",
      "Vata: 0.0099\n",
      "\n",
      "Predicted: Vata, Actual: Vata\n",
      "Output Probabilities:\n",
      "Kapha: 0.0000\n",
      "Pitta: 0.0000\n",
      "Vata: 1.0000\n",
      "\n",
      "Predicted: Kapha, Actual: Kapha\n",
      "Output Probabilities:\n",
      "Kapha: 0.9995\n",
      "Pitta: 0.0005\n",
      "Vata: 0.0000\n",
      "\n",
      "Predicted: Pitta, Actual: Pitta\n",
      "Output Probabilities:\n",
      "Kapha: 0.0021\n",
      "Pitta: 0.9906\n",
      "Vata: 0.0073\n",
      "\n",
      "Predicted: Kapha, Actual: Kapha\n",
      "Output Probabilities:\n",
      "Kapha: 1.0000\n",
      "Pitta: 0.0000\n",
      "Vata: 0.0000\n",
      "\n",
      "Predicted: Vata, Actual: Vata\n",
      "Output Probabilities:\n",
      "Kapha: 0.0029\n",
      "Pitta: 0.0075\n",
      "Vata: 0.9896\n",
      "\n",
      "Predicted: Vata, Actual: Vata\n",
      "Output Probabilities:\n",
      "Kapha: 0.0611\n",
      "Pitta: 0.0107\n",
      "Vata: 0.9283\n",
      "\n",
      "Predicted: Kapha, Actual: Kapha\n",
      "Output Probabilities:\n",
      "Kapha: 1.0000\n",
      "Pitta: 0.0000\n",
      "Vata: 0.0000\n",
      "\n",
      "Predicted: Vata, Actual: Vata\n",
      "Output Probabilities:\n",
      "Kapha: 0.0000\n",
      "Pitta: 0.0000\n",
      "Vata: 1.0000\n",
      "\n",
      "Predicted: Vata, Actual: Vata\n",
      "Output Probabilities:\n",
      "Kapha: 0.0001\n",
      "Pitta: 0.0010\n",
      "Vata: 0.9989\n",
      "\n",
      "Predicted: Vata, Actual: Vata\n",
      "Output Probabilities:\n",
      "Kapha: 0.0000\n",
      "Pitta: 0.0013\n",
      "Vata: 0.9986\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Lists to store predicted labels, actual labels, and probabilities\n",
    "predicted_labels = []\n",
    "actual_labels = []\n",
    "probabilities = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        probabilities_batch = F.softmax(outputs, dim=1)  # Apply softmax to get probabilities\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        # Convert predicted labels back to original categorical values\n",
    "        predicted_labels.extend(label_encoder.inverse_transform(predicted.numpy()))\n",
    "        actual_labels.extend(label_encoder.inverse_transform(labels.numpy()))\n",
    "        probabilities.extend(probabilities_batch.numpy())\n",
    "\n",
    "# Print predicted and actual labels along with output probabilities\n",
    "for predicted_label, actual_label, prob in zip(predicted_labels, actual_labels, probabilities):\n",
    "    print(f\"Predicted: {predicted_label}, Actual: {actual_label}\")\n",
    "    print(\"Output Probabilities:\")\n",
    "    for category, probability in zip(label_encoder.classes_, prob):\n",
    "        print(f\"{category}: {probability:.4f}\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dosha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
